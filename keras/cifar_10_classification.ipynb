{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_10_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eswens13/deep_learning/blob/master/keras/cifar_10_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z5Ar347Yyn5",
        "colab_type": "text"
      },
      "source": [
        "# CIFAR-10 Classifier/Autoencoder\n",
        "\n",
        "This notebook is an exploratory exercise in convolutional neural networks.  I will build a classifier for the CIFAR-10 image set and play around with network architecture, hyperparameters, visualization techniques, etc. to get hands-on experience coding convolutional neural networks in TensorFlow.\n",
        "\n",
        "I will also explore the differences between a classifier and an auto-encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adTggxluwJQT",
        "colab_type": "text"
      },
      "source": [
        "## Define Network Architecture\n",
        "\n",
        "First, we have to define a network architecture.  The code in the cell below has comments explaining the architecture of each of the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foT4cF9uqIpv",
        "colab_type": "code",
        "outputId": "0c6401c5-36d3-4314-aa25-25ec0c976c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Build the Keras model\n",
        "def get_keras_model():\n",
        "  \n",
        "  model = Sequential();\n",
        "  num_filters = 64\n",
        "  kernel_size = [3,3]\n",
        "  stride_size = [1,1]\n",
        "  pad_str = 'same'\n",
        "  format_str = 'channels_last'\n",
        "  act_str = 'relu'\n",
        "  \n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str, \\\n",
        "                   kernel_regularizer=l2(0.01)))\n",
        "  \n",
        "  pool_window = [2,2]\n",
        "  stride_size = [2,2]\n",
        "  pad_str = 'valid'\n",
        "  model.add(MaxPooling2D(pool_size=pool_window, \\\n",
        "                         strides=stride_size, \\\n",
        "                         padding=pad_str, \\\n",
        "                         data_format=format_str))\n",
        "\n",
        "  num_filters = 128\n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str, \\\n",
        "                   kernel_regularizer=l2(0.01)))\n",
        "  \n",
        "  model.add(MaxPooling2D(pool_size=pool_window, \\\n",
        "                         strides=stride_size, \\\n",
        "                         padding=pad_str, \\\n",
        "                         data_format=format_str))\n",
        "  \n",
        "  num_filters = 256\n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str, \\\n",
        "                   kernel_regularizer=l2(0.01)))\n",
        "  \n",
        "  pad_str = 'valid'\n",
        "  model.add(MaxPooling2D(pool_size=pool_window, \\\n",
        "                         strides=stride_size, \\\n",
        "                         padding=pad_str, \\\n",
        "                         data_format=format_str))\n",
        "  \n",
        "  model.add(Flatten(data_format=format_str))\n",
        "  \n",
        "  model.add(Dense(512, activation=act_str, use_bias=True))\n",
        "  model.add(Dense(128, activation=act_str, use_bias=True))\n",
        "  model.add(Dense(64, activation=act_str, use_bias=True))\n",
        "  \n",
        "  # Output Layer\n",
        "  model.add(Dense(10, activation=act_str, use_bias=True))\n",
        "  \n",
        "  # Compile the model.\n",
        "  adam = Adam(lr=1e-4)\n",
        "  model.compile(loss='categorical_crossentropy', \\\n",
        "                optimizer=adam, \\\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw4ep02TeBSX",
        "colab_type": "text"
      },
      "source": [
        "## Bring in Data\n",
        "\n",
        "In order to actually train the model, we need to bring in actual data.  Download the CIFAR-10 dataset and get it into a format that we can use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oswKFhE1LjQQ",
        "colab_type": "code",
        "outputId": "88a32cec-7a8c-4fdc-ed73-80527e6fe35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# I'm cheating and using Keras to import the dataset without having to do a lot\n",
        "# of processing myself.\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Change the labels to one-hot vectors.\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Examine what the data looks like.\n",
        "print(\"X_train shape: {}\".format(X_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))\n",
        "print(\"X_test shape: {}\".format(X_test.shape))\n",
        "print(\"y_test shape: {}\".format(y_test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 15s 0us/step\n",
            "X_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 10)\n",
            "X_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwSiivvF7DxC",
        "colab_type": "code",
        "outputId": "9561f0a3-8777-45dd-b83e-02d6d92a9783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Get a tool called ngrok to use as a tunnel between my local machine and the\n",
        "# Google Colab server. This will allow us to use TensorBoard to visualize\n",
        "# helpful metrics of the network.\n",
        "#\n",
        "# Tutorial:\n",
        "#   https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/\n",
        "#\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Now the ngrok exectuable is extracted to the current directory. Check to make\n",
        "# sure there is a log directory for Keras to use.\n",
        "cwd = os.getcwd()\n",
        "LOG_DIR = os.path.join(cwd, 'log')\n",
        "print(\"Log Dir: {}\".format(LOG_DIR))\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.system('mkdir -p {}'.format(LOG_DIR))\n",
        "\n",
        "# Run tensorboard in the background.\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "# Tell ngrok (in the background) to tunnel TensorBoard port 6006 to the outside\n",
        "# world.\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Get the URL that I can use to hook into TensorBoard from my local machine.\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-26 03:51:36--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.92.108.98, 54.152.127.232, 35.172.177.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.92.108.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  20.2MB/s    in 0.8s    \n",
            "\n",
            "2019-05-26 03:51:37 (20.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Log Dir: /content/log\n",
            "http://b6013c34.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-MOQUTGAbXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Keras callback so that it outputs to TensorBoard rather than this\n",
        "# console.\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "tbCallback = TensorBoard(log_dir=LOG_DIR, \\\n",
        "                         histogram_freq=1, \\\n",
        "                         write_graph=False, \\\n",
        "                         write_grads=True, \\\n",
        "                         batch_size=100, \\\n",
        "                         write_images=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGsrpneaF7Gb",
        "colab_type": "text"
      },
      "source": [
        "## Run Training\n",
        "\n",
        "Run the training loop for 100 batches of images (happens fairly fast) and investigate the effectiveness of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCSUGYpZk-Cp",
        "colab_type": "code",
        "outputId": "61fafb87-d9ba-48e8-e88f-544b28dfcbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "model = get_keras_model()\n",
        "model.fit(X_train, y_train, \\\n",
        "          epochs=20, batch_size=100, verbose=2, callbacks=[tbCallback], \\\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            " - 9s - loss: 5.9778 - acc: 0.1877 - val_loss: 3.3173 - val_acc: 0.2115\n",
            "Epoch 2/20\n",
            " - 5s - loss: 3.2307 - acc: 0.2481 - val_loss: 3.2132 - val_acc: 0.2106\n",
            "Epoch 3/20\n",
            " - 5s - loss: 3.5806 - acc: 0.1729 - val_loss: 4.5334 - val_acc: 0.1000\n",
            "Epoch 4/20\n",
            " - 5s - loss: 3.1086 - acc: 0.2375 - val_loss: 2.9132 - val_acc: 0.2425\n",
            "Epoch 5/20\n",
            " - 5s - loss: 3.1064 - acc: 0.2414 - val_loss: 2.7652 - val_acc: 0.2830\n",
            "Epoch 6/20\n",
            " - 5s - loss: 3.2539 - acc: 0.2295 - val_loss: 2.8670 - val_acc: 0.2606\n",
            "Epoch 7/20\n",
            " - 5s - loss: 3.3188 - acc: 0.2598 - val_loss: 5.6482 - val_acc: 0.1002\n",
            "Epoch 8/20\n",
            " - 5s - loss: 4.1149 - acc: 0.2030 - val_loss: 2.7001 - val_acc: 0.3163\n",
            "Epoch 9/20\n",
            " - 5s - loss: 2.8402 - acc: 0.2755 - val_loss: 2.5989 - val_acc: 0.3459\n",
            "Epoch 10/20\n",
            " - 5s - loss: 2.5791 - acc: 0.2877 - val_loss: 2.4670 - val_acc: 0.3456\n",
            "Epoch 11/20\n",
            " - 5s - loss: 2.5420 - acc: 0.2997 - val_loss: 2.4021 - val_acc: 0.3594\n",
            "Epoch 12/20\n",
            " - 5s - loss: 2.4094 - acc: 0.3464 - val_loss: 2.5559 - val_acc: 0.2459\n",
            "Epoch 13/20\n",
            " - 5s - loss: 4.1385 - acc: 0.1522 - val_loss: 2.7543 - val_acc: 0.1340\n",
            "Epoch 14/20\n",
            " - 5s - loss: 4.0585 - acc: 0.1600 - val_loss: 4.0167 - val_acc: 0.2287\n",
            "Epoch 15/20\n",
            " - 5s - loss: 2.8642 - acc: 0.2181 - val_loss: 2.4696 - val_acc: 0.2969\n",
            "Epoch 16/20\n",
            " - 5s - loss: 2.5067 - acc: 0.2312 - val_loss: 2.8186 - val_acc: 0.1000\n",
            "Epoch 17/20\n",
            " - 5s - loss: 2.6624 - acc: 0.1476 - val_loss: 2.4231 - val_acc: 0.2431\n",
            "Epoch 18/20\n",
            " - 5s - loss: 2.4390 - acc: 0.2303 - val_loss: 2.4696 - val_acc: 0.1859\n",
            "Epoch 19/20\n",
            " - 5s - loss: 2.3963 - acc: 0.2355 - val_loss: 2.4207 - val_acc: 0.1831\n",
            "Epoch 20/20\n",
            " - 5s - loss: 2.2690 - acc: 0.2895 - val_loss: 2.1846 - val_acc: 0.3119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d66acf4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky7MjyB3H9s5",
        "colab_type": "text"
      },
      "source": [
        "## Validate the Network\n",
        "\n",
        "Run the network against the validation data set to evaluate the classification effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE_JVXrOat0Y",
        "colab_type": "code",
        "outputId": "611f2801-cebe-4d90-c7c8-a2c420973c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"Accuracy: {}\".format(scores))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 72us/step\n",
            "Accuracy: [1.7493683261871338, 0.4116]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aH9pMCtmvqq",
        "colab_type": "text"
      },
      "source": [
        "# What Did I Learn?\n",
        "\n",
        "I had forgotten that \"epoch\" does not mean the same thing as \"batch\".  An epoch is one pass through all of the training data.  The batch size determines how often you update the network weights.  A larger batch sizes means less batches per epochs and less updates.  With a smaller batch size, more backward passes are performed and the epochs take a much longer time.\n",
        "\n",
        "Using TensorBoard (which is awesome to look at a lot of different metrics), I saw that my gradients for every layer are almost zero.  I'm guessing this is due to vanishing gradients."
      ]
    }
  ]
}