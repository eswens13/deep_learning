{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eswens13/deep_learning/blob/master/keras/mnist_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wDQcw_vddYZ",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Classifier\n",
        "\n",
        "Exploring an introductory deep learning problem to solidify some of the concepts I've learned thusfar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fs2Ocvidtd4",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Environment and Data\n",
        "\n",
        "We need to pull in the MNIST dataset and set up the environment so that we can checkpoint the model.  I have decided to use a utility called ngrok that acts as a tunnel between my local machine and the Colab server so that I can run TensorBoard and examine the training process.  This first cell spits out a URL where the TensorBoard visualizations can be viewed.\n",
        "\n",
        "Note that the samples in the MNIST dataset are grayscale so we have to artificially append a channel dimension to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON2SmTS4h6cF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "77e8d015-0ea4-4614-fd47-b44054021fc3"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "LINUX_BASE_PATH = '/content/drive/My\\ Drive/deep_learning'\n",
        "PYTHON_BASE_PATH = '/content/drive/My Drive/deep_learning'\n",
        "LINUX_MNIST_DIR = os.path.join(LINUX_BASE_PATH, 'mnist')\n",
        "PYTHON_MNIST_DIR = os.path.join(PYTHON_BASE_PATH, 'mnist')\n",
        "os.system('mkdir -p ' + LINUX_MNIST_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19beO0K8dnTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "273d8df1-00a7-475d-cbe0-c341339ae660"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# One-hot encode the labels.\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Need to give a \"channels\" dimension even though the images are grayscale.\n",
        "X_train = X_train[:,:,:,None]\n",
        "X_test = X_test[:,:,:,None]\n",
        "\n",
        "print(\"Data Shapes:\\n\\tX_train: {}\\n\\ty_train: {}\\n\\tX_test: {}\\n\\ty_test{}\"\\\n",
        "      .format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
        "\n",
        "import os\n",
        "\n",
        "# Get a tool called ngrok to use as a tunnel between my local machine and the\n",
        "# Google Colab server. This will allow us to use TensorBoard to visualize\n",
        "# helpful metrics of the network.\n",
        "#\n",
        "# Tutorial:\n",
        "#   https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/\n",
        "#\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Now the ngrok exectuable is extracted to the current directory. Check to make\n",
        "# sure there is a log directory for Keras to use.\n",
        "cwd = os.getcwd()\n",
        "LOG_DIR = os.path.join(cwd, 'log')\n",
        "print(\"Log Dir: {}\".format(LOG_DIR))\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.system('mkdir -p {}'.format(LOG_DIR))\n",
        "\n",
        "# Run tensorboard in the background.\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "# Tell ngrok (in the background) to tunnel TensorBoard port 6006 to the outside\n",
        "# world.\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Get the URL that I can use to hook into TensorBoard from my local machine.\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shapes:\n",
            "\tX_train: (60000, 28, 28, 1)\n",
            "\ty_train: (60000, 10)\n",
            "\tX_test: (10000, 28, 28, 1)\n",
            "\ty_test(10000, 10)\n",
            "--2019-05-31 04:59:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.206.36.121, 52.200.123.104, 3.214.163.243, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.206.36.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  20.2MB/s    in 0.8s    \n",
            "\n",
            "2019-05-31 04:59:18 (20.2 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Log Dir: /content/log\n",
            "https://b145d8c2.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahg7_guIfAlj",
        "colab_type": "text"
      },
      "source": [
        "## Network Architecture\n",
        "\n",
        "This next cell defines the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXl_oDAjfB6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Conv2D, Dense, Flatten, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Build the Keras model\n",
        "def get_keras_model():\n",
        "  \n",
        "  model = Sequential();\n",
        "  num_filters = 32\n",
        "  kernel_size = [3,3]\n",
        "  stride_size = [1,1]\n",
        "  pad_str = 'same'\n",
        "  format_str = 'channels_last'\n",
        "  act_str = 'relu'\n",
        "  \n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str))\n",
        "\n",
        "  num_filters = 64\n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str))\n",
        "  \n",
        "  num_filters = 64\n",
        "  model.add(Conv2D(num_filters, \\\n",
        "                   kernel_size, \\\n",
        "                   strides=stride_size, \\\n",
        "                   padding=pad_str, \\\n",
        "                   data_format=format_str, \\\n",
        "                   use_bias=True, \\\n",
        "                   activation=act_str))\n",
        "  \n",
        "  model.add(Flatten(data_format=format_str))\n",
        "  \n",
        "  model.add(Dense(512, activation=act_str, use_bias=True))\n",
        "  model.add(Dense(256, activation=act_str, use_bias=True))\n",
        "  model.add(Dense(128, activation=act_str, use_bias=True))\n",
        "  \n",
        "  # Output Layer\n",
        "  model.add(Dense(10, activation=act_str, use_bias=True))\n",
        "  \n",
        "  # Compile the model.\n",
        "  adam = Adam(lr=5e-4, decay=1e-7)\n",
        "  model.compile(loss='categorical_crossentropy', \\\n",
        "                optimizer=adam, \\\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4aR1TOyfSG2",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yZxWX0xe49H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4d844734-ca01-4193-8ce3-c1a187fa3720"
      },
      "source": [
        "# Create a Keras callback so that it outputs to TensorBoard rather than this\n",
        "# console.\n",
        "import keras.backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import load_model\n",
        "from datetime import datetime\n",
        "\n",
        "K.clear_session()\n",
        "tbCallback = TensorBoard(log_dir=LOG_DIR, \\\n",
        "                         histogram_freq=0, \\\n",
        "                         write_graph=False, \\\n",
        "                         write_grads=True, \\\n",
        "                         batch_size=100, \\\n",
        "                         write_images=False)\n",
        "\n",
        "\n",
        "\n",
        "# Run training on the model.\n",
        "total_epochs = 10\n",
        "curr_epochs = 0\n",
        "model = None\n",
        "full_model_path = \"\"\n",
        "\n",
        "while curr_epochs < total_epochs:\n",
        "  # Handle the checkpoint and first iteration cases.\n",
        "  if model == None:\n",
        "    if full_model_path == \"\":\n",
        "      # This means we're starting from scratch\n",
        "      model = get_keras_model()\n",
        "    else:\n",
        "      model = load_model(full_model_path)\n",
        "  \n",
        "  \n",
        "  model.fit(X_train, y_train, \\\n",
        "            epochs=10, batch_size=1000, verbose=2, callbacks=[tbCallback], \\\n",
        "            validation_data=(X_test, y_test))\n",
        "  \n",
        "  now = datetime.now()\n",
        "  curr_date_time = now.strftime(\"%m%d%Y_%H%M%S\")\n",
        "  model_file = \"{}.h5\".format(curr_date_time)\n",
        "  full_model_path = os.path.join(PYTHON_MNIST_DIR, model_file)\n",
        "\n",
        "  model.save(full_model_path)\n",
        "  del model\n",
        "  model = None\n",
        "  curr_epochs += 10"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 9s - loss: 3.0534 - acc: 0.4062 - val_loss: 2.4959 - val_acc: 0.6900\n",
            "Epoch 2/10\n",
            " - 9s - loss: 1.8255 - acc: 0.7309 - val_loss: 0.7322 - val_acc: 0.8961\n",
            "Epoch 3/10\n",
            " - 9s - loss: 1.0793 - acc: 0.6281 - val_loss: 0.7918 - val_acc: 0.8276\n",
            "Epoch 4/10\n",
            " - 9s - loss: 0.5773 - acc: 0.9053 - val_loss: 0.4311 - val_acc: 0.9330\n",
            "Epoch 5/10\n",
            " - 9s - loss: 0.4027 - acc: 0.9357 - val_loss: 0.3566 - val_acc: 0.9491\n",
            "Epoch 6/10\n",
            " - 9s - loss: 0.3651 - acc: 0.9405 - val_loss: 0.3398 - val_acc: 0.9512\n",
            "Epoch 7/10\n",
            " - 9s - loss: 0.8048 - acc: 0.7450 - val_loss: 0.5728 - val_acc: 0.9035\n",
            "Epoch 8/10\n",
            " - 9s - loss: 0.5593 - acc: 0.8834 - val_loss: 0.3842 - val_acc: 0.9365\n",
            "Epoch 9/10\n",
            " - 9s - loss: 0.5231 - acc: 0.8524 - val_loss: 0.3348 - val_acc: 0.9458\n",
            "Epoch 10/10\n",
            " - 9s - loss: 0.4303 - acc: 0.8895 - val_loss: 0.2850 - val_acc: 0.9460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pIvHuPTfbD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1c46dc34-e78a-4b40-81ae-0ea9c6f95adf"
      },
      "source": [
        "model_to_eval = load_model(full_model_path)\n",
        "scores = model_to_eval.evaluate(X_test, y_test)\n",
        "for i in range(len(model_to_eval.metrics_names)):\n",
        "  print(\"{}: {}\".format(model_to_eval.metrics_names[i], scores[i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 63us/step\n",
            "loss: 0.788991606760025\n",
            "acc: 0.8739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXOGOnXNEA7q",
        "colab_type": "text"
      },
      "source": [
        "## What Did I Learn?\n",
        "\n",
        "Doubled the size of my dense layers (three of them before output).  Increased batch size and learning rate.\n",
        "\n",
        "Took out max pooling after the two convolutional layers and that made a HUGE difference!  (Now ~93% accuracy.)\n",
        "\n",
        "####Current Best (05/30/19):  Consistently >93% from Epoch 3\n",
        "\n",
        "Conv 1: \\[3,3\\] kernel, 32 feature maps\n",
        "Conv 2: \\[3,3\\] kernel, 64 feature maps\n",
        "Conv 3: \\[3,3\\] kernel, 64 feature maps\n",
        "Flatten\n",
        "Dense:  512\n",
        "Dense:  256\n",
        "Dense:  128\n",
        "Output\n",
        "Adam Optimizer\n",
        "Learning Rate:  5e-4\n",
        "Decay:  1e-7\n",
        "\n",
        "###Archive\n",
        "\n",
        "####(05/30/19) 96% Accuracy, Epoch 9\n",
        "\n",
        "Conv 1: \\[3,3\\] kernel, 32 feature maps\n",
        "Conv 2: \\[3,3\\] kernel, 64 feature maps\n",
        "Conv 3: \\[3,3\\] kernel, 64 feature maps\n",
        "Flatten\n",
        "Dense:  512\n",
        "Dense:  256\n",
        "Dense:  128\n",
        "Output\n",
        "Adam Optimizer\n",
        "Learning Rate:  1e-4"
      ]
    }
  ]
}