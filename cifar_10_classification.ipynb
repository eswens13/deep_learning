{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_10_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eswens13/deep_learning/blob/master/cifar_10_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6z5Ar347Yyn5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 Classifier/Autoencoder\n",
        "\n",
        "This notebook is an exploratory exercise in convolutional neural networks.  I will build a classifier for the CIFAR-10 image set and play around with network architecture, hyperparameters, visualization techniques, etc. to get hands-on experience coding convolutional neural networks in TensorFlow.\n",
        "\n",
        "I will also explore the differences between a classifier and an auto-encoder."
      ]
    },
    {
      "metadata": {
        "id": "adTggxluwJQT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Network Architecture\n",
        "\n",
        "First, we have to define a network architecture.  The code in the cell below has comments explaining the architecture of each of the layers."
      ]
    },
    {
      "metadata": {
        "id": "foT4cF9uqIpv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def forward_pass_core(img_batch, classes):\n",
        "  \"\"\"\n",
        "  Defines the neural network architecture for CIFAR-10 classification.\n",
        "  \n",
        "  Parameters:\n",
        "    - img_batch: A batch of images to classify, as a Tensor object.\n",
        "                 Shape: (N x 32 x 32 x 3)\n",
        "    - classes:   The labels for each image, as a Tensor object.\n",
        "                 Shape: (N, 10)\n",
        "  \"\"\"\n",
        "  [N, H, W, C] = img_batch.get_shape().as_list()\n",
        "  \n",
        "  # Convolutional Layer 1:\n",
        "  #    - Input shape: (N, 32, 32, 3)\n",
        "  #    - 16 3x3 filters\n",
        "  #    - Zero-pad input to keep same feature map dimensions\n",
        "  #    - ReLU activation\n",
        "  #    - Output shape: (N, 32, 32, 16)\n",
        "  with tf.variable_scope(\"conv1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    filter_shape_conv1 = (3, 3)\n",
        "    filters_conv1 = 16\n",
        "    w_conv1 = tf.get_variable(\"W\", \\\n",
        "                              [filter_shape_conv1[0], \\\n",
        "                                filter_shape_conv1[1], \\\n",
        "                                C, \\\n",
        "                                filters_conv1], \\\n",
        "                              initializer=tf.random_normal_initializer)\n",
        "    b_conv1 = tf.get_variable(\"b\", \\\n",
        "                              [filters_conv1], \\\n",
        "                              initializer=tf.zeros_initializer)\n",
        "    strides = [1, 1, 1, 1]\n",
        "    padding = \"SAME\"\n",
        "    \n",
        "    conv_conv1 = tf.nn.conv2d(img_batch, w_conv1, strides, padding)\n",
        "    out1 = tf.nn.relu(conv_conv1 + b_conv1, name=scope.name)\n",
        "  \n",
        "  # Convolutional Layer 2:\n",
        "  #    - Input shape: (N, 32, 32, 16)\n",
        "  #    - 32 3x3 filters\n",
        "  #    - Lose a pixel off each side because of convolution.\n",
        "  #    - ReLU activation\n",
        "  #    - Output shape: (N, 30, 30, 32)\n",
        "  with tf.variable_scope(\"conv2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    filter_shape_conv2 = (3, 3)\n",
        "    filters_conv2 = 32\n",
        "    w_conv2 = tf.get_variable(\"W\", \\\n",
        "                              [filter_shape_conv2[0], \\\n",
        "                                filter_shape_conv2[1], \\\n",
        "                                filters_conv1, \\\n",
        "                                filters_conv2], \\\n",
        "                              initializer=tf.random_normal_initializer)\n",
        "    b_conv2 = tf.get_variable(\"b\", \\\n",
        "                              [filters_conv2], \\\n",
        "                              initializer=tf.zeros_initializer)\n",
        "    strides = [1, 1, 1, 1]\n",
        "    padding = \"VALID\"\n",
        "    \n",
        "    conv_conv2 = tf.nn.conv2d(out1, w_conv2, strides, padding)\n",
        "    out2 = tf.nn.relu(conv_conv2 + b_conv2, name=scope.name)\n",
        "  \n",
        "  # Convolutional Layer 3:\n",
        "  #    - Input shape: (N, 30, 30, 32)\n",
        "  #    - 64 5x5 filters\n",
        "  #    - Lose two pixels off each side because of convolution.\n",
        "  #    - ReLU activation\n",
        "  #    - Output shape: (N, 26, 26, 64)\n",
        "  with tf.variable_scope(\"conv3\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    filter_shape_conv3 = (5, 5)\n",
        "    filters_conv3 = 64\n",
        "    w_conv3 = tf.get_variable(\"W\", \\\n",
        "                              [filter_shape_conv3[0], \\\n",
        "                                filter_shape_conv3[1], \\\n",
        "                                filters_conv2, \\\n",
        "                                filters_conv3], \\\n",
        "                              initializer=tf.random_normal_initializer)\n",
        "    b_conv3 = tf.get_variable(\"b\", \\\n",
        "                              [filters_conv3], \\\n",
        "                              initializer=tf.zeros_initializer)\n",
        "    strides = [1, 1, 1, 1]\n",
        "    padding = \"VALID\"\n",
        "    \n",
        "    conv_conv3 = tf.nn.conv2d(out2, w_conv3, strides, padding)\n",
        "    out3 = tf.nn.relu(conv_conv3 + b_conv3, name=scope.name)\n",
        "  \n",
        "    # Since this is the last convolutional layer, we need to \"flatten\" the\n",
        "    # output into a one dimensional vector (well, really a one-dimensional\n",
        "    # vector per input image).\n",
        "    flat_out = tf.reshape(out3, [N, -1])\n",
        "    neurons_flat = flat_out.get_shape().as_list()[1]\n",
        "  \n",
        "  # Dense Layer 1:\n",
        "  #    - Input shape: (N, (26 * 26 * 64))\n",
        "  #    - Neurons: 512\n",
        "  #    - ReLU activation\n",
        "  #    - Ouput shape: (N, 512)\n",
        "  with tf.variable_scope(\"dense1\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    neurons_dense1 = 512\n",
        "    w_dense1 = tf.get_variable(\"W\", \\\n",
        "                               [neurons_flat, neurons_dense1], \\\n",
        "                               initializer=tf.random_normal_initializer)\n",
        "    b_dense1 = tf.get_variable(\"b\", \\\n",
        "                               [neurons_dense1], \\\n",
        "                               initializer=tf.zeros_initializer)\n",
        "    mul_dense1 = tf.matmul(flat_out, w_dense1)\n",
        "    out_dense1 = tf.nn.relu(mul_dense1 + b_dense1, name=scope.name)\n",
        "  \n",
        "  # Dense Layer 2:\n",
        "  #    - Input shape: (N, 512)\n",
        "  #    - Neurons: 128\n",
        "  #    - ReLU activation\n",
        "  #    - Ouput shape: (N, 128)\n",
        "  with tf.variable_scope(\"dense2\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    neurons_dense2 = 128\n",
        "    w_dense2 = tf.get_variable(\"W\", \\\n",
        "                               [neurons_dense1, neurons_dense2], \\\n",
        "                               initializer=tf.random_normal_initializer)\n",
        "    b_dense2 = tf.get_variable(\"b\", \\\n",
        "                                [neurons_dense2], \\\n",
        "                                initializer=tf.zeros_initializer)\n",
        "    mul_dense2 = tf.matmul(out_dense1, w_dense2)\n",
        "    out_dense2 = tf.nn.relu(mul_dense2 + b_dense2, name=scope.name)\n",
        "    \n",
        "  with tf.variable_scope(\"dense3\", reuse=tf.AUTO_REUSE) as scope:\n",
        "    n_classes = 10\n",
        "    w_dense3 = tf.get_variable(\"W\", \\\n",
        "                               [neurons_dense2, n_classes], \\\n",
        "                               initializer=tf.random_normal_initializer)\n",
        "    b_dense3 = tf.get_variable(\"b\", \\\n",
        "                                [n_classes], \\\n",
        "                                initializer=tf.zeros_initializer)\n",
        "    mul_dense3 = tf.matmul(out_dense2, w_dense3)\n",
        "    out_dense3 = mul_dense3 + b_dense3\n",
        "  \n",
        "  return out_dense3\n",
        "\n",
        "def forward_pass_train(img_batch, classes):\n",
        "  \n",
        "  # Get the output from the core of the network.\n",
        "  out_dense3 = forward_pass_core(img_batch, classes)\n",
        "  \n",
        "  # Add a Softmax layer with the cross entropy loss.\n",
        "  with tf.variable_scope(\"softmax\") as scope:\n",
        "    # The reduce_mean takes the mean in a given axis, thereby reducing the\n",
        "    # the dimensions of the tensor. If no axis is given, it takes the mean of\n",
        "    # the entire tensor.\n",
        "    #\n",
        "    # The softmax_cross_entropy_with_logits_v2 function calculates the softmax\n",
        "    # cross entropy loss without requiring us to convert the outputs of the last\n",
        "    # dense layer into probabilities first. (The 'logits' are raw outputs of the\n",
        "    # aforementioned outputs.)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2( \\\n",
        "                          labels=tf.stop_gradient(classes), logits=out_dense3))\n",
        "  \n",
        "  return loss\n",
        "\n",
        "def forward_pass_test(img_batch, classes):\n",
        "  \n",
        "  # Get the output from the core of the network.\n",
        "  out_dense3 = forward_pass_core(img_batch, classes)\n",
        "  \n",
        "  # Add a Softmax layer to get the core.\n",
        "  with tf.variable_scope(\"softmax\") as scope:\n",
        "    # The reduce_mean takes the mean in a given axis, thereby reducing the\n",
        "    # the dimensions of the tensor. If no axis is given, it takes the mean of\n",
        "    # the entire tensor.\n",
        "    #\n",
        "    # The softmax_cross_entropy_with_logits_v2 function calculates the softmax\n",
        "    # cross entropy loss without requiring us to convert the outputs of the last\n",
        "    # dense layer into probabilities first. (The 'logits' are raw outputs of the\n",
        "    # aforementioned outputs.)\n",
        "    softmax_out = tf.nn.softmax(logits=out_dense3)\n",
        "  \n",
        "  return softmax_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYdNDESBQPqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "\n",
        "Now that we've defined a network architecture, we need to set up a training loop to map how the network will be updated.  We need to choose an optimizer and set up backpropagation."
      ]
    },
    {
      "metadata": {
        "id": "rU1UppagQA_n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_batch(img_batch, labels, optimizer):\n",
        "  \"\"\"\n",
        "  Trains the network (forward pass followed by backpropagation) on a batch of\n",
        "  images.\n",
        "  \n",
        "  Parameters:\n",
        "    - img_batch: A batch of input images, as a Tensor object.\n",
        "                 Shape: (N, H, W, C)\n",
        "    - labels:    The labels for each input image in the batch, as a Tensor.\n",
        "                 Shape: (N, 10)\n",
        "    - optimizer: A TF Optimizer object that performs the backpropagation.\n",
        "  \"\"\"\n",
        "  loss = forward_pass_train(img_batch, labels)\n",
        "  opt = optimizer.minimize(loss)\n",
        "  return loss, opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8scLrYSed0AU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we simulate a single batch of data just to verify that forward and backward pass run without issue."
      ]
    },
    {
      "metadata": {
        "id": "9jGb86smxLMp",
        "colab_type": "code",
        "outputId": "b4649b51-5bd7-4f85-9e9c-b23959031e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Reset the graph so we don't have variable collisions when we re-run.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Simulate a batch of 32 images (each 32 x 32 pixels RBG)\n",
        "input_image = np.zeros((32, 32, 32, 3))\n",
        "\n",
        "# Simulate a batch of labels for the input images.\n",
        "input_labels = np.zeros((32, 10))\n",
        "for i in range(input_labels.shape[0]):\n",
        "  input_labels[i][i % 10] = 1\n",
        "\n",
        "# Create a placeholder for the input tensor (to be passed to the model).\n",
        "img = tf.placeholder(tf.float32, shape=input_image.shape)\n",
        "\n",
        "# Create a placeholder for the corresponding labels.\n",
        "img_labels = tf.placeholder(tf.float32, shape=input_labels.shape)\n",
        "\n",
        "# Create an optimizer that will manage the backpropagation.\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "\n",
        "# Run a single training loop.\n",
        "loss, backprop = train_batch(img, img_labels, optimizer)\n",
        "\n",
        "# Create the TF session and run the graph.\n",
        "with tf.Session() as sess:\n",
        "  # Initialize all the variables according to their initializers.\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # This call starts the chain of operations in the computation graph created\n",
        "  # above.\n",
        "  loss_output = sess.run(loss, feed_dict={img: input_image, \\\n",
        "                                          img_labels: input_labels})\n",
        "  \n",
        "  opt_output = sess.run(backprop, feed_dict={img: input_image, \\\n",
        "                                             img_labels: input_labels})\n",
        "  \n",
        "  print(\"Output: {}\".format(loss_output))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output: 2.3025851249694824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iw4ep02TeBSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bring in Data\n",
        "\n",
        "In order to actually train the model, we need to bring in actual data.  Download the CIFAR-10 dataset and get it into a format that we can use."
      ]
    },
    {
      "metadata": {
        "id": "oswKFhE1LjQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "88e36fd8-dcb7-4a61-f746-a58a8c039b84"
      },
      "cell_type": "code",
      "source": [
        "# I'm cheating and using Keras to import the dataset without having to do a lot\n",
        "# of processing myself.\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Change the labels to one-hot vectors.\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Examine what the data looks like.\n",
        "print(\"X_train shape: {}\".format(X_train.shape))\n",
        "print(\"y_train shape: {}\".format(y_train.shape))\n",
        "print(\"X_test shape: {}\".format(X_test.shape))\n",
        "print(\"y_test shape: {}\".format(y_test.shape))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 10)\n",
            "X_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fbLmbPTafmLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def training_loop(X, y, num_epochs=10, batch_size=128, learning_rate=1e-3):\n",
        "  \"\"\"\n",
        "  Trains a CNN on the CIFAR-10 dataset.\n",
        "  \n",
        "  Parameters:\n",
        "    - X:             The set of training samples to choose from. \n",
        "    - num_epochs:    The number of batches to process.\n",
        "    - num_samples:   The total number of samples to choose from.\n",
        "    - batch_size:    The number of samples in a training batch.\n",
        "    - learning_rate: The learning rate to use during backpropagation.\n",
        "  \"\"\"\n",
        "  num_samples = X.shape[0]\n",
        "  \n",
        "  # Reset the graph so we don't have variable collisions when we re-run.\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # Create a placeholder for the input tensor (to be passed to the model).\n",
        "  img = tf.placeholder(tf.float32, \\\n",
        "                       shape=(batch_size, X.shape[1], X.shape[2], X.shape[3]))\n",
        "\n",
        "  # Create a placeholder for the corresponding labels.\n",
        "  img_labels = tf.placeholder(tf.float32, shape=(batch_size, y.shape[1]))\n",
        "\n",
        "  # Create an optimizer that will manage the backpropagation.\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "\n",
        "  # Run a single training loop.\n",
        "  loss, backprop = train_batch(img, img_labels, optimizer)\n",
        "  \n",
        "  # Create a vector for the losses from each epoch.\n",
        "  loss_history = []\n",
        "\n",
        "  # Create the TF session and run the graph.\n",
        "  with tf.Session() as sess:\n",
        "    # Initialize all the variables according to their initializers.\n",
        "    sess.run(tf.global_variables_initializer())    \n",
        "  \n",
        "    for i in range(num_epochs):\n",
        "      # Randomly select a batch of images from the training set.\n",
        "      inds = np.random.randint(0, num_samples, batch_size)\n",
        "      batch = X[inds]\n",
        "      batch_labels = y[inds]\n",
        "      \n",
        "      # Get the loss using the current weights in the graph. This call starts\n",
        "      # the chain of operations in the computation graph created above.\n",
        "      epoch_loss = sess.run(loss, feed_dict={img: batch, \\\n",
        "                                             img_labels: batch_labels})\n",
        "      \n",
        "      # Run backpropagation on the graph.\n",
        "      sess.run(backprop, feed_dict={img: batch, img_labels: batch_labels})\n",
        "      \n",
        "      # Keep track of the loss.\n",
        "      loss_history.append(epoch_loss)\n",
        "\n",
        "  # Plot the loss over the whole training episode.\n",
        "  font_dict = { 'fontweight' : 'bold' }\n",
        "  plt.title(\"Training Loss\", fontdict=font_dict)\n",
        "  plt.plot(loss_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GGsrpneaF7Gb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Training\n",
        "\n",
        "Run the training loop for 100 batches of images (happens fairly fast) and investigate the effectiveness of the network."
      ]
    },
    {
      "metadata": {
        "id": "CCSUGYpZk-Cp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "b8a04bff-fd56-4fa6-bb1a-23cab0c33662"
      },
      "cell_type": "code",
      "source": [
        "training_loop(X_train, y_train, num_epochs=100)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd3a76c8f60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Ky7MjyB3H9s5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Validate the Network\n",
        "\n",
        "Run the network against the validation data set to evaluate the classification effectiveness."
      ]
    },
    {
      "metadata": {
        "id": "BBVXlbJqIGoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(X, y):\n",
        "  \"\"\"\n",
        "  Validates a CNN trained on the CIFAR-10 dataset.\n",
        "  \n",
        "  Parameters:\n",
        "    - X:    The set of validation samples. \n",
        "    - y:    The ground truth labels for the samples.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a placeholder for the input tensor (to be passed to the model).\n",
        "  img = tf.placeholder(tf.float32, shape=X.shape)\n",
        "\n",
        "  # Create a placeholder for the corresponding labels.\n",
        "  img_labels = tf.placeholder(tf.float32, shape=y.shape)\n",
        "  \n",
        "  # Create an operation for the test time forward pass.\n",
        "  test_op = forward_pass_test(img, img_labels)\n",
        "\n",
        "  # Create the TF session and run the graph.\n",
        "  with tf.Session() as sess:\n",
        "    # Create a batch of the same size as the network.\n",
        "    #batch_size = 128\n",
        "    #inds = np.random.randint(0, num_samples, batch_size)\n",
        "    #batch = X[inds]\n",
        "    #batch_labels = y[inds]\n",
        "    \n",
        "    # Get the loss using the current weights in the graph. This call starts\n",
        "    # the chain of operations in the computation graph created above.\n",
        "    labels = sess.run(test_op, feed_dict={img: X, img_labels: y})\n",
        "    \n",
        "    print(\"Output Shape: {}\".format(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OE_JVXrOat0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2717
        },
        "outputId": "b8918a6b-bb2f-4bcb-ec86-e6a0154df908"
      },
      "cell_type": "code",
      "source": [
        "validate(X_test, y_test)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv1/W\n\t [[{{node conv1/W/read}} = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv1/W)]]\n\t [[{{node softmax_2/Softmax/_3}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_76_softmax_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-1fa46a04e996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-064c277d7cc2>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Get the loss using the current weights in the graph. This call starts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# the chain of operations in the computation graph created above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output Shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv1/W\n\t [[node conv1/W/read (defined at <ipython-input-71-3a0980a078f3>:24)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv1/W)]]\n\t [[{{node softmax_2/Softmax/_3}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_76_softmax_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'conv1/W/read', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-76-abb383ed79af>\", line 1, in <module>\n    training_loop(X_train, y_train, num_epochs=100)\n  File \"<ipython-input-75-a8da4239d22c>\", line 27, in training_loop\n    loss, backprop = train_batch(img, img_labels, optimizer)\n  File \"<ipython-input-72-6fa18980660b>\", line 15, in train_batch\n    loss = forward_pass_train(img_batch, labels)\n  File \"<ipython-input-71-3a0980a078f3>\", line 108, in forward_pass_train\n    out_dense3 = forward_pass_core(img_batch, classes)\n  File \"<ipython-input-71-3a0980a078f3>\", line 24, in forward_pass_core\n    w_conv1 = tf.get_variable(\"W\",                               [filter_shape_conv1[0],                                 filter_shape_conv1[1],                                 C,                                 filters_conv1],                               initializer=tf.random_normal_initializer)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1487, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1237, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 540, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 492, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 922, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1491, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 81, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3454, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv1/W\n\t [[node conv1/W/read (defined at <ipython-input-71-3a0980a078f3>:24)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv1/W)]]\n\t [[{{node softmax_2/Softmax/_3}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_76_softmax_2/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "UKgICByiayEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}