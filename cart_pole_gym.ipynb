{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cart_pole_gym.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eswens13/deep_learning/blob/master/cart_pole_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imx47lXUlfzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVz_RqD31W3V",
        "colab_type": "code",
        "outputId": "768df800-02f1-493f-89a0-01a4291693fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install pyglet==1.3.2"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqrAs0b7sK_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "#gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwTnlh3gmkQW",
        "colab_type": "code",
        "outputId": "31465488-eeed-4db5-86a8-f936483b3eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "# You can just ignore the warning 'xdpyinfo was not found' for now."
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvYHQI8Fq7LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions to enable video recording of gym environment and displaying\n",
        "# it. To enable video, just do:\n",
        "#   \"env = wrap_env(env)\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh4JWSVir3lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Where ENV_NAME is the environment that are using from Gym, eg 'CartPole-v0'\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = wrap_env(gym.make(ENV_NAME)) #wrapping the env to render as a video"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YnvwQJ00FZ0",
        "colab_type": "code",
        "outputId": "0eb35376-68c5-431c-9595-531a2edfcc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Check out the action and observation spaces.\n",
        "#\n",
        "# According to the GitHub\n",
        "# (https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py),\n",
        "# the discrete action space is defined as follows:\n",
        "#\n",
        "#     0 - Push the cart to the left\n",
        "#     1 - Push the cart to the right\n",
        "#\n",
        "# and the continuous observation (state) space is defined as follows:\n",
        "#\n",
        "#     Num\tObservation               Min             Max\n",
        "#       0\tCart Position             -4.8            4.8\n",
        "#       1\tCart Velocity             -Inf            Inf\n",
        "#       2\tPole Angle                 -24 deg        24 deg\n",
        "#       3\tPole Velocity At Tip      -Inf            Inf\n",
        "#\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n",
            "Box(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay4IYvlkx1ag",
        "colab_type": "code",
        "outputId": "5900cc7f-149a-436e-a0fc-c7d9124d573f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "# The video that is generated ends very quickly. After examining the code for\n",
        "# the CartPole environment, I found that the threshold on the angle of the pole\n",
        "# before the environment considers itself done is 12 degrees. Obviously, with a\n",
        "# random selection policy, this will be hit very quickly.\n",
        "for i_episode in range(20):\n",
        "  env.reset()\n",
        "  for t in range(100):\n",
        "      env.render()\n",
        "      action = env.action_space.sample() # Choose randomly between push left or\n",
        "                                         # push right\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      # Just take a look a the first observation:\n",
        "      if (i_episode == 0) and (t == 0):\n",
        "        print(\"Initial State:\")\n",
        "        print(\"\\tCart Position: {}\\n\\tCart Velocity: {}\".format( \\\n",
        "                  observation[0], observation[1]))\n",
        "        print(\"\\tPole Angle: {}\\n\\tPole Velocity: {}\\n\".format( \\\n",
        "                  observation[2], observation[3]))\n",
        "\n",
        "      if done:\n",
        "        if (i_episode == 0):\n",
        "          print(\"Episode finished after {} timesteps\\n\".format(t+1))\n",
        "          print(\"Final State:\")\n",
        "          print(\"\\tCart Position: {}\\n\\tCart Velocity: {}\".format( \\\n",
        "                  observation[0], observation[1]))\n",
        "          print(\"\\tPole Angle: {}\\n\\tPole Velocity: {}\\n\".format( \\\n",
        "                  observation[2], observation[3]))\n",
        "        break;\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial State:\n",
            "\tCart Position: 0.01152971081264392\n",
            "\tCart Velocity: 0.24208906720693307\n",
            "\tPole Angle: 0.02920878008462663\n",
            "\tPole Velocity: -0.3111074074918091\n",
            "\n",
            "Episode finished after 20 timesteps\n",
            "\n",
            "Final State:\n",
            "\tCart Position: -0.0817165090993933\n",
            "\tCart Velocity: -0.7507447671846365\n",
            "\tPole Angle: 0.22781983875317552\n",
            "\tPole Velocity: 1.567791629084882\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tVUSd1Pk8Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(x):\n",
        "  \"\"\"\n",
        "  Defines the neural network architecture used for the reinforcement learning\n",
        "  agent. This is a very simple architecture consisting of only a few dense\n",
        "  layers followed by a softmax at the output.\n",
        "\n",
        "  Inputs to the neural network are the current state observation as well as the\n",
        "  last observation. Each observation consists of:\n",
        "    Cart Position         [-4.8, 4.8]\n",
        "    Cart Velocity         (-Inf, Inf)\n",
        "    Pole Angle            [-24 deg, 24 deg]\n",
        "    Pole Velocity At Tip  (-Inf, Inf)\n",
        "  \n",
        "  Arguments:\n",
        "    x: The tensor of current and previous state observations.\n",
        "  \n",
        "  Returns:\n",
        "    probs: The one hot encoded action vector (i.e. the decision for which action\n",
        "           to take next).\n",
        "  \"\"\"\n",
        "  y = tf.layers.dense(x, 8, activation='relu')\n",
        "  y = tf.layers.dense(y, 8, activation='relu')\n",
        "  y = tf.layers.dense(y, 8, activation='relu')\n",
        "  y = tf.layers.dense(y, 2)\n",
        "  probs = tf.nn.softmax(y)\n",
        "  one_hot = tf.one_hot(tf.argmax(probs, axis=1), 2)\n",
        "  return one_hot\n",
        "\n",
        "def get_loss(obs_batch):\n",
        "  \"\"\"\n",
        "  Computes the loss value for a batch of observations. This loss will be used to\n",
        "  update the network weights after a given number of time steps have elapsed.\n",
        "\n",
        "  Arguments:\n",
        "    obs_batch:  A tensor of observations.\n",
        "  \n",
        "  Returns:\n",
        "    loss:   A scalar loss value used to update the network weights.\n",
        "  \"\"\"\n",
        "  # Currently, we just use the absolute value of the pole angle as the loss\n",
        "  # value.\n",
        "  angle = obs_batch[:,2]\n",
        "  loss = tf.reduce_sum(tf.abs(angle))\n",
        "  return loss\n",
        "\n",
        "def get_update_op(lr, curr_step, max_steps, obs_batch):\n",
        "  \"\"\"\n",
        "  Gets the update operation that applies gradients to the network weights.\n",
        "\n",
        "  Arguments:\n",
        "    lr:         The initial learning rate value (decay is applied in this\n",
        "                function).\n",
        "    curr_step:  The current time step in the simulation.\n",
        "    max_steps:  The maximum time step in the simulation.\n",
        "    obs_batch:  The tensor of most recent observations.\n",
        "  \n",
        "  Returns:\n",
        "    update_op:  The update operation to be used to update the network weights.\n",
        "  \"\"\"\n",
        "  # Apply learning rate decay.\n",
        "  curr_lr = tf.train.polynomial_decay( \\\n",
        "                      lr, curr_step, max_steps, end_learning_rate=1e-5)\n",
        "  \n",
        "  # Use the Adam optimizer to backpropagate loss.\n",
        "  optimizer = tf.train.AdamOptimizer(curr_lr)\n",
        "\n",
        "  # Create the TF op for backprop.\n",
        "  update_op = optimizer.minimize(get_loss(obs_batch))\n",
        "  return update_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCjmf6yVurdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Instantiate the computational graph.\n",
        "  x = tf.placeholder(tf.float32, shape=(10, 4))\n",
        "  probs = forward_pass(x)\n",
        "\n",
        "  # Initialize the variables after they've been allocated.\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "\n",
        "  # There is a warning that is output about bound methods not being able to be\n",
        "  # transformed. According to this thread on GitHub, this is a benign warning.\n",
        "  #   https://github.com/tensorflow/addons/issues/94\n",
        "  action = sess.run(probs, feed_dict={x: inputs})\n",
        "  print(\"Actions: {}\".format(probs, one_hot))\n",
        "\n",
        "total_steps = 0\n",
        "max_steps = 10000\n",
        "learning_rate = 1e-3\n",
        "done = False\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session as sess:\n",
        "  # Create variables, placeholders, ops, etc.\n",
        "  curr_obs = tf.placeholder(tf.float32, shape=(1,4))\n",
        "  prev_obs = tf.placeholder(tf.float32, shape=(1,4))\n",
        "  curr_reward = tf.placeholder(tf.float32, shape(1,1))\n",
        "  curr_state = tf.concat([curr_obs, prev_obs, curr_reward], axis=1)\n",
        "\n",
        "  choose_action_op = choose_action(curr_state)\n",
        "\n",
        "  curr_step_val = tf.placeholder(tf.int32, shape=(1,))\n",
        "  max_step_val = tf.constant(max_steps, dtype=tf.int32)\n",
        "  obs_batch = tf.placeholder(tf.float32, shape=(batch_size, 4))\n",
        "  update_op = get_update_op(learning_rate, \\\n",
        "                            curr_step_val, \\\n",
        "                            max_step_val, \\\n",
        "                            obs_batch)\n",
        "\n",
        "  # Initialize variables.\n",
        "  sess.run(tf.initialize_all_variables())\n",
        "  reward_signal = 0.0\n",
        "  gamma = 0.75\n",
        "\n",
        "  # Run the simulation\n",
        "  while total_steps < max_steps:\n",
        "    env.reset()\n",
        "    prev_observation = [0.0, 0.0, 0.0, 0.0]\n",
        "    curr_observation = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "\n",
        "        # Get the next action.\n",
        "        action = sess.run(choose_action_op, \\\n",
        "                          feed_dict={curr_obs: curr_observation, \\\n",
        "                                     prev_obs: prev_observation, \\\n",
        "                                     curr_reward: reward_signal})\n",
        "        prev_obs = curr_obs\n",
        "        curr_obs, reward, done, _ = env.step(action)\n",
        "        reward_signal = (gamma * reward) + ((1.0 - gamma) * reward_signal)\n",
        "        total_steps += 1\n",
        "\n",
        "  env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}